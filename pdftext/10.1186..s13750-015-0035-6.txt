Davoudi et al. Environmental Evidence (2015) 4:9
DOI 10.1186/s13750-015-0035-6
  COMMENTARY                                                                                                                                         Open Access
Judging research quality to support evidence-
informed environmental policy
Simin Davoudi1*, Gemma Harper2, Judith Petts3 and Sarah Whatmore4
  Keywords: Research quality, Social sciences, Systematic reviews, Evidence-informed
In August 2005, PLoS Medicine published an essay by                                      to help policy makers to navigate research bias’ [7] and
John Ioannidis called: ‘Why most published research                                      suggested the need for establishing third-party certified
findings are false’ [1]. Since then, the paper has been                                  auditors and international auditing standards that grade
viewed over a million times possibly because of its pro-                                 scientific studies or even journals. A case has also been
vocative title, but probably because of growing concerns                                 made for adopting healthcare best practice quality assess-
about the reliability of scientific publications and dimin-                              ment tools for environmental science [8]. Others have
ishing confidence in the peer review process to deliver                                  suggested the use of ‘formal consensus methods’ such as
effective quality control. Most recently such concerns                                   Delphi techniques to achieve better quality control [9]. Dr
have been supported by a number of high profile cases                                    Ioannidis (mentioned above) has already institutionalised
of publication retraction, for example the withdrawal of                                 these ideas by launching in early 2014 a Meta-Research
some statements from articles published in the British                                   Innovation Centre at Stanford University. METRICS’ mis-
Medical Journal regarding the adverse side effects of                                    sion is ‘to undertake rigorous evaluation of research prac-
Statins, a cholesterol-reducing drug [2]. A series of arti-                              tices and find ways to optimize the reproducibility and
cles in the Lancet has suggested that some $200 billion                                  efficiency of scientific investigations’ [10]. These efforts
(estimated to be 80% of the world’s spend on medical                                     point to a growing call for the tightening of peer review,
research) was wasted on ‘studies that were flawed in their                               or even dispensing with it, in favour of post-publication
design, redundant, never published or poorly reported’ [3].                              evaluation in the form of appended comments.
Moreover, ‘when a prominent medical journal ran research                                     We share the concerns raised by other commentators
past other experts in the field, it found that most of the re-                           about the reliability of research and the evidence it pro-
viewers failed to spot mistakes it had deliberately inserted                             duces, and support the efforts to promote quality assur-
into papers, even after being told they were being tested’                               ance. However, our motivation for writing this article is
[4]. In the field of environmental studies too, concerns                                 to raise concerns about the perceived validity and value
have been raised about ‘the limited effectiveness of peer-                               of social science evidence (compared with scientific out-
review as a quality-control mechanism’ [5].                                              puts from the physical, natural, engineering, and medical
   The actual and perceived unreliability of scientific re-                              sciences) in interdisciplinary research. Interdisciplinarity
ports and papers is particularly problematic for govern-                                 is particularly relevant in the field of environmental pol-
ments’ scientific advisors who operate in the ‘messy’                                    icy and management which often grapples with multiple
world of policy making. Here even credible evidence is                                   questions that demand diverse research methods from
rarely the only influential factor [6] and policy (and evi-                              both social and natural sciences. Defined broadly, the
dence) contention, scientific uncertainty and even ignor-                                social sciences study societal processes and peoples’ lived
ance pose significant challenges. The Chief Scientific                                   experiences as these shape, and are shaped by, the world
Advisor for the UK government’s Department for Environment,                              around them. Understanding what people, individually
Food and Rural Affairs has called ‘for an auditing process                               and in various forms of association with others, think
                                                                                         and do poses unique research challenges. Studying soci-
* Correspondence: simin.davoudi@ncl.ac.uk                                                ety involves not only the objective system under scru-
1
 School of Architecture, Planning and Landscape, Newcastle University,                   tiny, but also the subjective system of scrutiny itself
Claremont Tower, Newcastle upon Tyne NE1 7RU, UK
Full list of author information is available at the end of the article                   (known as double hermeneutic). In consequence, social
                                            © 2015 Davoudi et al.; licensee BioMed Central. This is an Open Access article distributed under the terms of the Creative
                                            Commons Attribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and
                                            reproduction in any medium, provided the original work is properly credited. The Creative Commons Public Domain
                                            Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article,
                                            unless otherwise stated.

Davoudi et al. Environmental Evidence (2015) 4:9                                                                 Page 2 of 3
science disciplines have developed a variety of quantita-    example, while statistics can tell us the voting patterns
tive and qualitative research techniques adapted to these    of a given social group in a general election, they do not
challenges. They draw on data generated by a variety of      explain why the group and importantly the individuals
methods including statistical analyses, survey question-     within it, voted as they did. As with all sciences, what
naires, in-depth interviews, participant observation, and    causes something to happen in a particular case may not
group discussions. Some of these methods and the cri-        necessarily be explained by the number of times we ob-
teria appropriate for evaluating the reliability of the      serve it happening. Finding out ‘how’ and ‘why’ people
evidence that they generate may be unfamiliar to other       vote as they do necessitates an understanding of what vot-
scientific fields.                                           ing for a particular outcome means to the individual voter.
   We are concerned that a desire to set universally         Such understanding requires a mixture of complementary
applicable ‘kite marks’ and ‘gold standards’ may risk        quantitative and qualitative (or Q2) methods. Thus, ap-
undermining: an appreciation of the complementarity          propriateness should be the first test of quality control.
of different methods (within and between the quantitative    Once the appropriateness of the method is established,
and qualitative); the importance of adopting an inclusive    criteria relevant to that method can be drawn upon to as-
definition of evidence; the diversity of research designs    sess its quality and distinguish between, for example, a
and methods; and, the significance of ‘fitness for purpose’  high and low quality RCT, or a high and low quality focus
in research design, conduct and reporting. There appears     group. This means that before asking whether this re-
to be a tendency to consider qualitative methods as some-    search is valid, we should be asking what ‘this research is
how inferior to experimental or quantitative methods a       valid for’ [15]. We would be amongst the first to acknow-
priori [11]. From our experience, sometimes decision         ledge that there is low quality social science as well as low
makers question evidence that is based on analysis of nar-   quality natural science but, no method can be judged bet-
rative, discussion and commentary and use statistical rep-   ter or worse than another in isolation from the research
resentativeness and reproducibility as the primary criteria  questions they aim to address. Accordingly, it is essential
for assessing the quality of research. These tendencies are  that we avoid the tendency to assess the quality of re-
not random instances; they are indicative of power rela-     search methods by a universal set of criteria or worse
tions within and beyond science and embedded in the fab-     even, to assess qualitative methods by the same criteria
ric of knowledge itself. Arguing against these tendencies is developed for and used in quantitative methods (such as
not new. It has a long pedigree in the field of environmen-  the statistical validity of the participants sample size for a
tal research and policy-making [12] whose interdisciplin-    focus group).
ary nature demands that diverse framings of the problem         There is a growing body of literature on criteria and
and multiple methods of investigation typically come to-     checklists for assessing quality in social science research
gether and challenge each other in producing new ways of     [16,17]. These have been applied to single research pro-
knowing.                                                     jects and syntheses of qualitative research, as well as sys-
   In this context, evidence must be understood broadly      tematic reviews similar to those conducted by Cochrane
to encompass the insights from the natural, physical and     and Campbell Collaborations and the Collaboration for
social sciences and provide space for ‘a measured array      Environmental Evidence. Reports have suggested that
of contrasting specialist views’ [13]. While our focus       there are over one hundred sets of proposals on quality
here is on research, we believe that tacit and experiential  in qualitative research [18]. However, there appears to be
knowledge by which ‘much of the world’s work of prob-        few attempts to develop method-specific approaches. Fur-
lem solving is accomplished’ [14] should also be in-         thermore, in selecting papers for inclusion in the system-
cluded in the definition of evidence. Similarly, quality     atic reviews ‘consensus about which aspects of design,
should be defined inclusively and the mechanisms and cri-    execution, analysis and description are most crucial is yet
teria used to judge it should reflect the diversity of re-   to be reached’ [19]. Furthermore, there is even a lack of
search methods and paradigms. This means that the            consensus about whether such reviews are appropriate for
criteria used to assess the quality of, for example, rando-  studies using qualitative methods whose assessment
mised controlled trials (RCT) may not be suitable for        involves an iterative process and does not follow the often
assessing qualitative methods.                               linear approach used in experimental and quantitative
   The key point is that applying the same criteria univer-  research [20]. One area on which both social and natural
sally to all types of research is imprudent. The approach    scientists agree is the acknowledgment that assessing the
to quality control should start by asking which method       quality of evidence is a subjective process and involves
or mixture of methods are most appropriate for answer-       judgment. In the context of systematic reviews, structured
ing the research questions and the research project’s        approaches (such as checklists and tools) have long been
intended uses. The validity and credibility of the method    proposed as a means of assessing the quality of research
depends fundamentally on its fitness for purpose. For        reports and reducing subjectivity. However, a comparison

Davoudi et al. Environmental Evidence (2015) 4:9                                                                                                     Page 3 of 3
of structured approaches and ‘unprompted judgement’                           Received: 12 November 2014 Accepted: 11 March 2015
has shown that although the former ‘may sensitise re-
viewers to aspects of research practice’, they do ‘not appear                 References
more likely to produce a higher level of agreement between                    1. Ioannidis J. Why most published research findings are false. PLoS Medicine.
or within reviewers’ [21]. It is also important to note that                      2005;2(8):696–701.
                                                                              2. British Medical Journal: Editorials, Adverse effects of statins, 2014, 348:g3563
there is a wide range of methods for synthesising qualita-                        published 15 May, available at http://www.bmj.com/content/348/bmj.g3306
tive research. Barnet-Page and Thomas [22], for example,                          accessed 11 August 2014
have identified ten different methods spanning across the                     3. The Economist: Combating bad science, Metaphysicians, 2014, 15 March,
                                                                                  p.78 available at http://www.economist.com/news/science-and-technology/
“realist – idealist” epistemological spectrum and each with                       21598944-sloppy-researchers-beware-new-institute-has-you-its-sights-
their own criteria for quality assessment. It is therefore                        metaphysicians accessed 11 August 2014
important that in undertaking systematic reviews of quali-                    4. The Economist: How science goes wrong? 2013, 19 October, Leader section,
                                                                                  available at http://www.economist.com/news/leaders/21588069-scientific-
tative research, attention is paid to the suitability of the                      research-has-changed-world-now-it-needs-change-itself-how-science-goes-
criteria for not only quality assessment but also the syn-                        wrong accessed 11 August 2014
thesis method itself.                                                         5. Bilotta G, Milner A, Boyd I. Quality assessment tools for evidence from
                                                                                  environmental science. Environ Evid. 2014;3(14):1–14. p. 1.
   To summarise, the main messages of this commentary                         6. Davoudi S. Evidence-based planning: rhetoric and reality. DisP: The Planning
are as follows:                                                                   Review. 2006;165(2):14–25.
                                                                              7. Boyd I. A standard for policy-relevant science. Nature. 2013;501:159–60.
                                                                                  p.159.
    Evidence for environmental policy should be defined                      8. Bilotta G, Milner A, Boyd I. Quality assessment tools for evidence from
       broadly and inclusively to incorporate the insights                        environmental science. Environ Evid. 2014;3(14):1–14.
       from all sciences.                                                     9. Sutherland WJ. Review by quality not quantity for better policy. Nature.
                                                                                  2013;503:167–68. p.167.
    There is a diversity of social scientific research                       10. Available at http://med.stanford.edu/metrics/ accessed 7/8/2014
       methods, each with its own specific contributions to                   11. Veltri GA, Lim J, Miller R. More than meets the eye: the contribution of
       environmental decision making.                                             qualitative research to evidence-based policy making. Innovation: The
                                                                                  European Journal of Social Sciences Research. 2014;27(1):1–4.
    Mechanisms and criteria for judging research quality                     12. Burgess J, Goldsmith B, Harrison C. Pale shadows for policy: reflections on
       should take account of such diversity and be fit for                       the Greenwich open space project. Stud Qual Meth. 1990;2:141–67.
       purpose.                                                               13. Stirling A. Keep it complex. Nature. 2010;468:1029–31. p. 1030.
                                                                              14. Lindblom CE, Cohen D. Unusable knowledge: Social sciences and social
    To make the best of social sciences their                                    problem solving. New Haven CT: Yale University; 1979. p. 91.
       contributions should be fully integrated at the                        15. Garside R. Should we appraise the quality of qualitative research reports
       beginning into environmental policy development                            for systematic reviews, and if so, how? Innovat Eur J Soc Sci Res.
                                                                                  2014;27(1):67–9. p.76.
       and interdisciplinary research.                                        16. HM Treasury. Quality in qualitative evaluation: a framework for assessing
                                                                                  research evidence (supplementary Magenta Book guidance). London: HM
Abbreviations                                                                     Treasury; 2012.
RCT: Randomised controlled trials.                                            17. National Centre for Social Research. Quality in qualitative education: a
                                                                                  framework for assessing research evidence. London: National Centre for
Authors’ contributions                                                            Social Research / UK Cabinet Office; 2003.
SD drafted and revised the manuscript. JP, GH and SW commented on and         18. NHSCRD (NHS Centre for Reviews and Dissemination): Undertaking
improved the draft manuscript. All authors read and approved the final            systematic reviews of research on effectiveness: CRD’s guidance for those
manuscript.                                                                       carrying out or commissioning reviews NHS Centre for Reviews and
                                                                                  Dissemination, 2001, York University, York YO10 5DD (CRD Report 4: second
Authors’ information                                                              edition), p. 221. Available at: http://www.york.ac.uk/inst/crd/report4.htm
SD is Professor of Environmental Policy and Planning at Newcastle University.     accessed 7/8/2014
GH is Chief Social Scientist at the Department for Environment, Food and      19. Garside R. Should we appraise the quality of qualitative research reports
Rural Affairs (Defra). JP is Professor of Environmental Risk Management at        for systematic reviews, and if so, how? Innovat Eur J Soc Sci Res.
Southampton University and member of DEFRA Science Advisory Council.              2014;27(1):67–9. p.68.
SW is Professor of Environment and Public Policy at Oxford University and     20. Freeman M, de Marrais K, Pressie J, Roulston K, St Pierre EA. Standards of
member of DEFRA Science Advisory Council. All authors are members of the          evidence in qualitative research: An introduction to discourses. Educational
Defra and DECC (Department for Energy and Climate Change) Social Science          Researcher. 2007;36:25–32.
Expert Panel.                                                                 21. Dixon-Woods M, Sutton A, Shaw R, Miller T, Smith J, Young B, et al.
                                                                                  Appraising qualitative research for inclusion in systematic reviews: a
Acknowledgement                                                                   quantitative and qualitative comparison of three methods. Journal of Health
We would like to thank members of the Department for Environment, Food            Service Research Policy. 2007;12(1):42–7. p. 46.
and Rural Affairs (Defra) and the Department for Energy and Climate Change    22. Barnett-Page E, Thomas J. Methods for the synthesis of qualitative of
(DECC) Social Science Expert Panel, Defra Social Research Group, Professor        research: a critical review. BMC Med Res Meth. 2009;9:59.
Andy Stirling, and two anonymous reviewers for their insightful comments.
The views expressed here are solely our responsibility.
Author details
1
 School of Architecture, Planning and Landscape, Newcastle University,
Claremont Tower, Newcastle upon Tyne NE1 7RU, UK. 2Department for
Environment, Food and Rural Affairs, 5C, Noble House, 17 Smith Square,
London SW1P 3JR, UK. 3Vice-Chancellor’s Office, University of Southampton,
Highfield, Southampton SO17 1BJ, UK. 4Keble College, Oxford OX1 3PG, UK.

